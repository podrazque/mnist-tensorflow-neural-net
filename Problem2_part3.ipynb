{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.reset_default_graph()\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "tf.set_random_seed(777)\n",
    "np.random.seed(444)\n",
    "import seaborn as sns\n",
    "import pickle, gzip\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_python2():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as fd:\n",
    "        #u = pickle._Unpickler(fd)\n",
    "        #u.encoding = 'latin1'\n",
    "        train_set, valid_set, test_set = pickle.load(fd)\n",
    "        #u.load()\n",
    "        return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_python3():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as fd:\n",
    "        u = pickle._Unpickler(fd)\n",
    "        u.encoding = 'latin1'\n",
    "        train_set, valid_set, test_set = u.load()\n",
    "        return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(sys.version[0]=='3'):\n",
    "    train_set, valid_set, test_set = get_data_python3()\n",
    "elif(sys.version[0]=='2'):\n",
    "    train_set, valid_set, test_set = get_data_python2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and val\n",
    "train_set = (np.concatenate([train_set[0], valid_set[0]], axis=0), \n",
    "                np.concatenate([train_set[1], valid_set[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Size of training set:', 60000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set:\", len(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Size of each training set item (28x28 image 1-d array): ', 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of each training set item (28x28 image 1-d array): \", len(train_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping each 784 1-d array into 28x28 2-d array\n",
    "Train_data = train_set[0].reshape(60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Initialization ##########################################################\n",
    "num_epochs = 50\n",
    "LR = 0.0001\n",
    "nunit = 32\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # Convolution Layers\n",
    "    'c1': tf.get_variable('W1', shape=(5,5,1,20), \\\n",
    "            initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'c2': tf.get_variable('W2', shape=(5,5,20,50), \\\n",
    "            initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'c3': tf.get_variable('W3', shape=(4,4,50,500), \\\n",
    "            initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'c4': tf.get_variable('W4', shape=(1,1,500,10), \\\n",
    "            initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    \n",
    "    # Dense Layers\n",
    "    #'d1': tf.get_variable('W5', shape=(7*7*32,28*28), \n",
    "     #       initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    #'out': tf.get_variable('W6', shape=(128,n_classes), \n",
    "     #       initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}\n",
    "biases = {\n",
    "    # Convolution Layers\n",
    "    'c1': tf.get_variable('B1', shape=(20), initializer=tf.zeros_initializer()),\n",
    "    'c2': tf.get_variable('B2', shape=(50), initializer=tf.zeros_initializer()),\n",
    "    'c3': tf.get_variable('B3', shape=(500), initializer=tf.zeros_initializer()),\n",
    "    'c4': tf.get_variable('B4', shape=(10), initializer=tf.zeros_initializer()),\n",
    "    \n",
    "    # Dense Layers\n",
    "    #'d1': tf.get_variable('B5', shape=(128), initializer=tf.zeros_initializer()),\n",
    "    #'out': tf.get_variable('B6', shape=(n_classes), initializer=tf.zeros_initializer()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_1(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "def conv2d_2(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "def conv2d_3(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net_dropout(data, weights, biases, training=False):\n",
    "    # Convolution layers\n",
    "    conv1 = conv2d_1(data, weights['c1'], biases['c1']) \n",
    "    dropout1 = tf.nn.dropout(conv1,0.8)\n",
    "    pool1 = tf.nn.max_pool(dropout1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')     \n",
    "    conv2 = conv2d_1(pool1, weights['c2'], biases['c2'])\n",
    "    dropout2 = tf.nn.dropout(conv2,0.8)\n",
    "    pool2 = tf.nn.max_pool(dropout2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID') \n",
    "    conv3 = conv2d_2(pool2, weights['c3'], biases['c3']) \n",
    "    conv4 = conv2d_3(conv3, weights['c4'], biases['c4']) \n",
    "    \n",
    "    out = conv4 # [10]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "def conv_net(data, weights, biases, training=False):\n",
    "    # Convolution layers\n",
    "    conv1 = conv2d_1(data, weights['c1'], biases['c1']) \n",
    "    batch_mean1, batch_var1 = tf.nn.moments(conv1,[0])\n",
    "    scale1 = tf.Variable(tf.ones([20]))\n",
    "    beta1 = tf.Variable(tf.zeros([20]))\n",
    "    normalized_layer_1 = tf.nn.batch_normalization(conv1, batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
    "    pool1 = tf.nn.max_pool(normalized_layer_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')   \n",
    "    conv2 = conv2d_1(pool1, weights['c2'], biases['c2'])\n",
    "    batch_mean2, batch_var2 = tf.nn.moments(conv2,[0])\n",
    "    scale2 = tf.Variable(tf.ones([50]))\n",
    "    beta2 = tf.Variable(tf.zeros([50]))\n",
    "    normalized_layer_2 = tf.nn.batch_normalization(conv2, batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "    pool2 = tf.nn.max_pool(normalized_layer_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID') \n",
    "    conv3 = conv2d_2(pool2, weights['c3'], biases['c3']) \n",
    "    batch_mean3, batch_var3 = tf.nn.moments(conv3,[0])\n",
    "    scale3 = tf.Variable(tf.ones([500]))\n",
    "    beta3 = tf.Variable(tf.zeros([500]))\n",
    "    normalized_layer_3 = tf.nn.batch_normalization(conv3, batch_mean3,batch_var3,beta3,scale3,epsilon)\n",
    "    conv4 = conv2d_3(normalized_layer_3, weights['c4'], biases['c4']) \n",
    "    out = conv4 # [10]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "logits = conv_net(Xtrain, weights, biases)\n",
    "#logits = conv_net_dropout(Xtrain, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "ytrain = tf.placeholder(tf.float32, shape=(None, n_classes))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, \n",
    "                                                                 labels=ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.1,momentum=0.9)\n",
    "#optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#GradientDescentOptimizer\n",
    "#MomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "tests_labels=tf.placeholder(tf.float32, shape=(None, n_classes))\n",
    "test_predictions = tf.nn.softmax(conv_net(test_images, weights, biases))\n",
    "#acc,acc_op = tf.metrics.accuracy(predictions=tf.argmax(test_predictions,1), \n",
    " #                                labels=tests_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skp\n",
    "enc = skp.OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(train_set[1].reshape(-1,1))\n",
    "Ytrain_data=enc.transform(train_set[1].reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0003394070645173391)\n",
      "1.5276014\n",
      "(60, 0.0002961809694766998)\n",
      "1.474844\n",
      "(120, 0.0002943173468112946)\n",
      "1.4689826\n",
      "(180, 0.0002936561604340871)\n",
      "1.4660041\n",
      "(240, 0.0002933264056841532)\n",
      "1.4647366\n",
      "(300, 0.0002930129547913869)\n",
      "1.4639701\n",
      "(360, 0.0002928788681825002)\n",
      "1.4636068\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size=5000\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "for epochs in range(num_epochs):\n",
    "    res_avg = 0.\n",
    "    for i in range(0, 60000, batch_size):\n",
    "        feed_dict={Xtrain: Train_data[i:i+batch_size].reshape(-1,28,28,1), ytrain:Ytrain_data[i:i+batch_size].reshape(-1,10) }\n",
    "        _,res = sess.run([train_op,loss], feed_dict=feed_dict)\n",
    "        res_avg += np.sum(res)\n",
    "    if epochs%5==0.:\n",
    "        print(int(60000/batch_size)*epochs, res_avg/60000)\n",
    "        print(np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Test_data = test_set[0].reshape(10000,28,28)\n",
    "pred=sess.run(test_predictions,feed_dict={test_images:Test_data.reshape(-1,28,28,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=[]\n",
    "for i in range(10000):\n",
    "    pred_label.append(np.argmax(pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=np.array(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "accuracy = sklearn.metrics.accuracy_score(test_set[1], pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
